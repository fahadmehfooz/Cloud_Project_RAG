{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cebf419e-3ba2-4475-bb64-4cff238c0428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence_transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: faiss-gpu in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: rank_bm25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rank_bm25) (1.26.4)\n",
      "Requirement already satisfied: dotenv in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dotenv) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install faiss-gpu\n",
    "!pip install rank_bm25\n",
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "415cdb6c-54aa-4df0-8093-da086c5de5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import hashlib\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from sentence_transformers import CrossEncoder\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import hashlib\n",
    "import json\n",
    "import requests\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eacbc40-2018-4206-9b61-f50b8efa7e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fa1286f-369f-4967-be1b-43d32fec47a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 05:47:20,821 - __main__ - INFO - Loading chunks from chunks_5000.pkl\n",
      "2025-04-28 05:47:21,215 - __main__ - INFO - Loading embeddings from embeddings_5000.pkl\n",
      "2025-04-28 05:47:21,225 - __main__ - INFO - Creating restaurant recommender\n",
      "2025-04-28 05:47:21,226 - __main__ - INFO - Initializing Restaurant Recommender\n",
      "2025-04-28 05:47:23,099 - __main__ - INFO - Creating FAISS index with 5000 embeddings\n",
      "2025-04-28 05:47:23,134 - __main__ - INFO - Saving all components\n",
      "2025-04-28 05:47:23,135 - __main__ - INFO - Saving TF-IDF vectorizer to saved_objects/tfidf_vectorizer.pkl\n",
      "2025-04-28 05:47:23,173 - __main__ - INFO - Saving TF-IDF matrix to saved_objects/tfidf_matrix.pkl\n",
      "2025-04-28 05:47:23,203 - __main__ - INFO - Saving FAISS index to saved_objects/faiss_index.bin\n",
      "2025-04-28 05:47:23,243 - __main__ - INFO - Saving FAISS index mapping to saved_objects/faiss_index_to_chunk.pkl\n",
      "2025-04-28 05:47:23,281 - __main__ - INFO - Saving processed chunks to saved_objects/processed_chunks.pkl\n",
      "2025-04-28 05:47:23,339 - __main__ - INFO - All components saved successfully!\n",
      "2025-04-28 05:47:23,340 - __main__ - INFO - Testing recommendation for query: spicy vegetarian food in san francisco\n",
      "2025-04-28 05:47:23,340 - __main__ - INFO - Generating recommendation for: spicy vegetarian food in san francisco\n",
      "2025-04-28 05:47:23,342 - __main__ - INFO - Searching for: spicy vegetarian food in san francisco\n",
      "2025-04-28 05:47:23,342 - __main__ - INFO - TF-IDF retrieval for query: spicy vegetarian food in san francisco\n",
      "2025-04-28 05:47:23,388 - __main__ - INFO - Generating embedding for query\n",
      "2025-04-28 05:47:23,515 - __main__ - INFO - Vector retrieval with embedding\n",
      "2025-04-28 05:47:23,520 - __main__ - INFO - Generating response for query: spicy vegetarian food in san francisco\n",
      "2025-04-28 05:47:28,585 - __main__ - INFO - Recommendation for 'spicy vegetarian food in san francisco':\n",
      "2025-04-28 05:47:28,587 - __main__ - INFO - Response: Based on your query for spicy vegetarian food in San Francisco, I recommend Bac Lieu Restaurant, which offers two excellent options:\n",
      "\n",
      "1. R16 Vermicelli in Vietnamese Yellow Curry with Fried Tofu: This is an ideal match for your request. It's a spicy vegetarian dish with a fixed spice level, featuring tofu and yellow curry over vermicelli. The price is mid-range, and it's located in San Francisco.\n",
      "\n",
      "2. R5 Vegetarian Pho: Another great vegetarian option with a housemade vegetable broth, including ingredients like tofu, bean sprouts, and cilantro. While not specifically marked as spicy, it provides a flavorful vegetarian alternative.\n",
      "\n",
      "Both dishes are at Bac Lieu Restaurant, offer vegetarian Vietnamese cuisine, and are priced in the mid-range. The curry dish, in particular, stands out for its guaranteed spiciness and vegetarian profile.\n",
      "2025-04-28 05:47:28,588 - __main__ - INFO - Found 3 results in 5.24 seconds\n",
      "2025-04-28 05:47:28,589 - __main__ - INFO - Result 1: bac lieu restaurant - r5 vegetarian pho - Score: 0.0300\n",
      "2025-04-28 05:47:28,590 - __main__ - INFO - Result 2: bac lieu restaurant - r16 vermicelli in vietnamese yellow curry with fried tofu spicy - Score: 0.0299\n",
      "2025-04-28 05:47:28,590 - __main__ - INFO - Result 3: kazan - poke roll - Score: 0.0167\n",
      "2025-04-28 05:47:28,591 - __main__ - INFO - Total processing time: 7.77 seconds\n",
      "2025-04-28 05:47:28,592 - __main__ - INFO - Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(\"Credentials\")  \n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    service_name = \"bedrock-runtime\",\n",
    "    region_name=\"us-east-2\",\n",
    "    aws_access_key_id = os.getenv(\"aws_access_key_id\"), \n",
    "    aws_secret_access_key = os.getenv(\"aws_secret_access_key\")\n",
    ")\n",
    "\n",
    "import pickle\n",
    "import faiss\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import hashlib\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    \"\"\"Class for handling document preprocessing tasks\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def standardize_chunks(chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Ensure all chunks have a consistent structure with required fields.\"\"\"\n",
    "        standardized_chunks = []\n",
    "        for chunk in chunks:\n",
    "            # Create a new chunk to avoid modifying the original\n",
    "            new_chunk = chunk.copy()\n",
    "            \n",
    "            # Fix text field if it's a dictionary\n",
    "            if isinstance(new_chunk.get('text'), dict) and 'text' in new_chunk['text']:\n",
    "                new_chunk['text'] = new_chunk['text']['text']\n",
    "                \n",
    "            # Ensure chunk_id exists if missing\n",
    "            if 'chunk_id' not in new_chunk:\n",
    "                chunk_text = str(new_chunk['text'])\n",
    "                new_chunk['chunk_id'] = int(hashlib.md5(chunk_text.encode()).hexdigest(), 16) % 10000000\n",
    "                \n",
    "            # Ensure metadata exists\n",
    "            if 'metadata' not in new_chunk:\n",
    "                new_chunk['metadata'] = {}\n",
    "                \n",
    "            standardized_chunks.append(new_chunk)\n",
    "        \n",
    "        return standardized_chunks\n",
    "\n",
    "class TextRetriever:\n",
    "    \"\"\"Class for text-based retrieval methods\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_tfidf_vectorizer(chunks: List[Dict]) -> Tuple[TfidfVectorizer, np.ndarray]:\n",
    "        \"\"\"Create and fit TF-IDF vectorizer for document corpus.\"\"\"\n",
    "        # Ensure chunks have the correct structure\n",
    "        chunks = DocumentPreprocessor.standardize_chunks(chunks)\n",
    "        \n",
    "        # Extract text corpus\n",
    "        corpus = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # Create and fit vectorizer\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        \n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        return vectorizer, tfidf_matrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def retrieve_with_tfidf(\n",
    "        query: str, \n",
    "        chunks: List[Dict], \n",
    "        vectorizer: TfidfVectorizer, \n",
    "        tfidf_matrix: np.ndarray, \n",
    "        top_n: int = 50\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Retrieve documents using TF-IDF similarity.\"\"\"\n",
    "        logger.info(f\"TF-IDF retrieval for query: {query}\")\n",
    "        \n",
    "        # Ensure chunks have the correct structure\n",
    "        chunks = DocumentPreprocessor.standardize_chunks(chunks)\n",
    "        \n",
    "        try:\n",
    "            # Transform query\n",
    "            query_vec = vectorizer.transform([query])\n",
    "            \n",
    "            # Calculate similarity\n",
    "            cosine_sim = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "            \n",
    "            # Sort and filter results\n",
    "            scored_chunks = sorted(\n",
    "                [(i, score) for i, score in enumerate(cosine_sim)],\n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:top_n]\n",
    "            \n",
    "            # Format results\n",
    "            results = [\n",
    "                {\n",
    "                    \"metadata\": chunks[i][\"metadata\"],\n",
    "                    \"text\": chunks[i][\"text\"],\n",
    "                    \"chunk_id\": chunks[i][\"chunk_id\"],\n",
    "                    \"score\": float(score),\n",
    "                    \"retrieval_method\": \"tfidf\"\n",
    "                } \n",
    "                for i, score in scored_chunks\n",
    "            ]\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in TF-IDF retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "def get_titan_embeddings(text, dimensions=512):\n",
    "    \"\"\"Generate embeddings using Amazon Titan model.\"\"\"\n",
    "    logger.info(f\"Generating embedding for query\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize Bedrock client\n",
    "        bedrock_client = boto3.client(\n",
    "            service_name = \"bedrock-runtime\",\n",
    "            region_name=\"us-east-2\",\n",
    "            aws_access_key_id = os.getenv(\"aws_access_key_id\"), \n",
    "            aws_secret_access_key = os.getenv(\"aws_secret_access_key\")\n",
    "        )        \n",
    "        body = json.dumps({\n",
    "            \"inputText\": text,\n",
    "            \"dimensions\": dimensions,\n",
    "            \"normalize\": True\n",
    "        })\n",
    "        \n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"*/*\",\n",
    "            body=body\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        \n",
    "        # Extract the embedding\n",
    "        embedding = response_body['embedding']\n",
    "        return np.array(embedding, dtype=np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating Titan embeddings: {e}\")\n",
    "        # Generate fallback random embedding\n",
    "        embedding = np.random.rand(dimensions).astype('float32')\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        return embedding\n",
    "\n",
    "class VectorRetriever:\n",
    "    \"\"\"Class for vector-based retrieval methods\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n",
    "        \"\"\"Create a FAISS index from embeddings.\"\"\"\n",
    "        logger.info(f\"Creating FAISS index with {len(embeddings)} embeddings\")\n",
    "        \n",
    "        # Ensure proper typing\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        # Validate dimensions\n",
    "        if embeddings.ndim != 2:\n",
    "            raise ValueError(f\"Expected 2D array, got {embeddings.ndim}D\")\n",
    "            \n",
    "        # Create index\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    @staticmethod\n",
    "    def retrieve_with_embeddings(\n",
    "        query_embedding: np.ndarray,\n",
    "        faiss_index: faiss.IndexFlatL2, \n",
    "        faiss_index_to_chunk: Dict[int, Dict], \n",
    "        top_n: int = 20\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Retrieve documents using embedding similarity.\"\"\"\n",
    "        logger.info(f\"Vector retrieval with embedding\")\n",
    "        \n",
    "        try:\n",
    "            # Ensure query embedding is 2D\n",
    "            query_embedding_2d = query_embedding.reshape(1, -1).astype('float32')\n",
    "            \n",
    "            # Search index\n",
    "            distances, indices = faiss_index.search(query_embedding_2d, top_n)\n",
    "            \n",
    "            # Format vector results\n",
    "            results = []\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx >= 0 and idx < len(faiss_index_to_chunk):\n",
    "                    chunk = faiss_index_to_chunk[idx]\n",
    "                    if isinstance(chunk.get('text'), dict) and 'text' in chunk['text']:\n",
    "                        chunk['text'] = chunk['text']['text']\n",
    "                        \n",
    "                    results.append({\n",
    "                        \"metadata\": chunk,\n",
    "                        \"text\": chunk[\"text\"],\n",
    "                        \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                        \"score\": float(1 / (1 + distances[0][i])),\n",
    "                        \"retrieval_method\": \"vector\"\n",
    "                    })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in vector retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Class for hybrid retrieval combining multiple methods\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _deduplicate_results(results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Deduplicate results by chunk_id or custom identifier.\"\"\"\n",
    "        seen = set()\n",
    "        deduplicated = []\n",
    "        \n",
    "        for res in results:\n",
    "            # Determine a unique identifier\n",
    "            if 'item_id' in res[\"metadata\"]:\n",
    "                uid = res[\"metadata\"][\"item_id\"]\n",
    "            elif 'restaurant_id' in res[\"metadata\"] and 'menu_item' in res[\"metadata\"]:\n",
    "                uid = f\"{res['metadata']['restaurant_id']}_{res['metadata']['menu_item']}\"\n",
    "            else:\n",
    "                uid = res['chunk_id']\n",
    "                \n",
    "            if uid not in seen:\n",
    "                seen.add(uid)\n",
    "                deduplicated.append(res)\n",
    "                \n",
    "        return deduplicated\n",
    "    \n",
    "    @staticmethod\n",
    "    def reciprocal_rank_fusion(results_list: List[List[Dict]], k: float = 60.0) -> List[Dict]:\n",
    "        \"\"\"Combine multiple result lists using reciprocal rank fusion.\"\"\"\n",
    "        # Handle empty results\n",
    "        if not results_list or all(not results for results in results_list):\n",
    "            return []\n",
    "            \n",
    "        # Deduplicate by unique identifier across all result lists\n",
    "        all_results = []\n",
    "        for results in results_list:\n",
    "            all_results.extend(results)\n",
    "            \n",
    "        deduplicated = HybridRetriever._deduplicate_results(all_results)\n",
    "        \n",
    "        # Calculate RRF scores\n",
    "        item_scores = {}\n",
    "        \n",
    "        # Process each result list\n",
    "        for rank_group_idx, result_list in enumerate(results_list):\n",
    "            # Get scores by rank position\n",
    "            for rank, item in enumerate(result_list):\n",
    "                item_id = item['chunk_id']\n",
    "                if item_id not in item_scores:\n",
    "                    item_scores[item_id] = 0.0\n",
    "                    \n",
    "                # RRF formula: 1 / (k + rank)\n",
    "                item_scores[item_id] += 1.0 / (k + rank)\n",
    "        \n",
    "        # Apply RRF scores to deduplicated results\n",
    "        for item in deduplicated:\n",
    "            item['rrf_score'] = item_scores.get(item['chunk_id'], 0.0)\n",
    "            item['original_score'] = item['score']  # Preserve original score\n",
    "            item['score'] = item['rrf_score']  # Replace with fusion score\n",
    "            \n",
    "        # Sort by fusion score\n",
    "        return sorted(deduplicated, key=lambda x: x['rrf_score'], reverse=True)\n",
    "\n",
    "class LLMResponseGenerator:\n",
    "    \"\"\"Class for generating responses using LLM through AWS Bedrock\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_context(results: List[Dict]) -> str:\n",
    "        \"\"\"Format retrieval results into a context string for the LLM.\"\"\"\n",
    "        context = \"Here are some relevant restaurant items:\\n\\n\"\n",
    "        \n",
    "        for i, res in enumerate(results):\n",
    "            meta = res.get('metadata', {})\n",
    "            context += f\"[{i+1}] {meta.get('restaurant_name', 'Unknown')}: {meta.get('menu_item', 'Unknown Item')}\\n\"\n",
    "            context += f\"Description: {meta.get('menu_description', 'No description available')}\\n\"\n",
    "            context += f\"Price: {meta.get('price_tier', 'unknown').capitalize()}\\n\"\n",
    "            \n",
    "            if meta.get('cuisine_types'):\n",
    "                context += f\"Cuisine: {', '.join(meta.get('cuisine_types', []))}\\n\"\n",
    "                \n",
    "            if meta.get('ingredients'):\n",
    "                context += f\"Ingredients: {', '.join(meta.get('ingredients', []))}\\n\"\n",
    "                \n",
    "            context += f\"Location: {meta.get('location', 'Unknown')}\\n\\n\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    @staticmethod\n",
    "    def call_bedrock_claude(prompt: str) -> str:\n",
    "        \"\"\"Call AWS Bedrock Claude model to generate a response using the specified payload structure.\"\"\"\n",
    "        try:\n",
    "            # Initialize Bedrock client\n",
    "            bedrock_client = boto3.client(\n",
    "                service_name = \"bedrock-runtime\",\n",
    "                region_name=\"us-east-2\",\n",
    "                aws_access_key_id = os.getenv(\"aws_access_key_id\"), \n",
    "                aws_secret_access_key = os.getenv(\"aws_secret_access_key\")\n",
    "            )            \n",
    "            \n",
    "            # Create payload using the provided structure\n",
    "            payload = {\n",
    "                \"modelId\": os.getenv(\"inference_profile\"),\n",
    "                \"body\": json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 500,\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"system\": \"\"\"You are a helpful restaurant recommendation assistant. \n",
    "                    Provide concise, helpful recommendations based on the retrieved information.\n",
    "                    Focus on menu items, price, location, and cuisine types that best match the query.\"\"\",\n",
    "                    \"messages\": [{\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        }]\n",
    "                    }]\n",
    "                })\n",
    "            }\n",
    "            \n",
    "            # Call the model using invoke_model\n",
    "            response = bedrock_client.invoke_model(**payload)\n",
    "            \n",
    "            # Parse the response\n",
    "            result = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
    "            generated_text = result.get(\"content\", [{}])[0].get(\"text\", \"\").strip()\n",
    "            \n",
    "            return generated_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calling AWS Bedrock Claude: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def simple_fallback_response(query: str, results: List[Dict]) -> str:\n",
    "        \"\"\"Generate a simple response without calling an LLM.\"\"\"\n",
    "        if not results:\n",
    "            return f\"I couldn't find any restaurant recommendations for '{query}'. Could you provide more details or try a different search?\"\n",
    "        \n",
    "        response = f\"Here are some restaurant recommendations for '{query}':\\n\\n\"\n",
    "        \n",
    "        for i, result in enumerate(results[:3]):\n",
    "            meta = result.get('metadata', {})\n",
    "            response += (\n",
    "                f\"{i+1}. {meta.get('restaurant_name', 'Unknown')}\\n\"\n",
    "                f\"   - Menu Item: {meta.get('menu_item', 'N/A')}\\n\"\n",
    "                f\"   - Price: {meta.get('price_tier', '').capitalize()}\\n\"\n",
    "            )\n",
    "            \n",
    "            if meta.get('cuisine_types'):\n",
    "                response += f\"   - Cuisine: {', '.join(meta.get('cuisine_types', []))}\\n\"\n",
    "                \n",
    "            if meta.get('ingredients'):\n",
    "                response += f\"   - Ingredients: {', '.join(meta.get('ingredients', []))}\\n\"\n",
    "                \n",
    "            response += f\"   - Location: {meta.get('location', 'Unknown')}\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_response(query: str, results: List[Dict]) -> str:\n",
    "        \"\"\"Generate a response to the query using retrieved results.\"\"\"\n",
    "        logger.info(f\"Generating response for query: {query}\")\n",
    "        \n",
    "        if not results:\n",
    "            return f\"I couldn't find any restaurant recommendations for '{query}'. Could you provide more details or try a different search?\"\n",
    "        \n",
    "        # Format the context and prompt for the LLM\n",
    "        context = LLMResponseGenerator.format_context(results)\n",
    "        prompt = (\n",
    "            f\"Based on the following restaurant information, provide a helpful response to this query: '{query}'\\n\\n\"\n",
    "            f\"{context}\\n\"\n",
    "            f\"Provide a concise recommendation that highlights the best matches for the query. \"\n",
    "            f\"Include details about the food, price, and location.\"\n",
    "        )\n",
    "        \n",
    "        # Try calling the LLM first\n",
    "        llm_response = LLMResponseGenerator.call_bedrock_claude(prompt)\n",
    "        \n",
    "        # If the LLM call fails, use the fallback\n",
    "        if llm_response is None:\n",
    "            logger.warning(\"LLM call failed, using fallback response generator\")\n",
    "            return LLMResponseGenerator.simple_fallback_response(query, results)\n",
    "        \n",
    "        return llm_response\n",
    "\n",
    "class RestaurantRecommender:\n",
    "    \"\"\"Main class for the restaurant recommendation system\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks: List[Dict], embeddings: np.ndarray):\n",
    "        \"\"\"Initialize the recommender system with chunks and embeddings.\"\"\"\n",
    "        logger.info(\"Initializing Restaurant Recommender\")\n",
    "        \n",
    "        # Standardize document chunks\n",
    "        self.chunks = DocumentPreprocessor.standardize_chunks(chunks)\n",
    "        \n",
    "        # Create TF-IDF components\n",
    "        self.vectorizer, self.tfidf_matrix = TextRetriever.build_tfidf_vectorizer(self.chunks)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self.faiss_index = VectorRetriever.create_faiss_index(embeddings)\n",
    "        \n",
    "        # Create index mapping\n",
    "        self.faiss_index_to_chunk = {\n",
    "            i: {\n",
    "                **chunk[\"metadata\"],\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"chunk_id\": chunk[\"chunk_id\"]\n",
    "            } for i, chunk in enumerate(self.chunks)\n",
    "        }\n",
    "    \n",
    "    def save_components(self, save_dir: str = 'saved_objects'):\n",
    "        \"\"\"Save all fitted components to disk.\"\"\"\n",
    "        # Create save directory if it doesn't exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Define paths for saved components\n",
    "        vectorizer_path = os.path.join(save_dir, 'tfidf_vectorizer.pkl')\n",
    "        tfidf_matrix_path = os.path.join(save_dir, 'tfidf_matrix.pkl')\n",
    "        faiss_index_path = os.path.join(save_dir, 'faiss_index.bin')\n",
    "        faiss_mapping_path = os.path.join(save_dir, 'faiss_index_to_chunk.pkl')\n",
    "        chunks_save_path = os.path.join(save_dir, 'processed_chunks.pkl')\n",
    "        \n",
    "        # Save TF-IDF vectorizer\n",
    "        logger.info(f\"Saving TF-IDF vectorizer to {vectorizer_path}\")\n",
    "        with open(vectorizer_path, 'wb') as f:\n",
    "            pickle.dump(self.vectorizer, f)\n",
    "        \n",
    "        # Save TF-IDF matrix\n",
    "        logger.info(f\"Saving TF-IDF matrix to {tfidf_matrix_path}\")\n",
    "        with open(tfidf_matrix_path, 'wb') as f:\n",
    "            pickle.dump(self.tfidf_matrix, f)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        logger.info(f\"Saving FAISS index to {faiss_index_path}\")\n",
    "        faiss.write_index(self.faiss_index, faiss_index_path)\n",
    "        \n",
    "        # Save FAISS index mapping\n",
    "        logger.info(f\"Saving FAISS index mapping to {faiss_mapping_path}\")\n",
    "        with open(faiss_mapping_path, 'wb') as f:\n",
    "            pickle.dump(self.faiss_index_to_chunk, f)\n",
    "        \n",
    "        # Save processed chunks\n",
    "        logger.info(f\"Saving processed chunks to {chunks_save_path}\")\n",
    "        with open(chunks_save_path, 'wb') as f:\n",
    "            pickle.dump(self.chunks, f)\n",
    "        \n",
    "        logger.info(\"All components saved successfully!\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Perform hybrid search with the given query.\"\"\"\n",
    "        logger.info(f\"Searching for: {query}\")\n",
    "        \n",
    "        # Step 1: Retrieve with TF-IDF\n",
    "        tfidf_results = TextRetriever.retrieve_with_tfidf(\n",
    "            query, self.chunks, self.vectorizer, self.tfidf_matrix, 50\n",
    "        )\n",
    "        \n",
    "        # Step 2: Generate query embedding\n",
    "        try:\n",
    "            query_embedding = get_titan_embeddings(query)\n",
    "            \n",
    "            # Step 3: Search with vector embedding\n",
    "            vector_results = VectorRetriever.retrieve_with_embeddings(\n",
    "                query_embedding, \n",
    "                self.faiss_index, \n",
    "                self.faiss_index_to_chunk, \n",
    "                20\n",
    "            )\n",
    "            \n",
    "            # Step 4: Combine results with RRF\n",
    "            results_list = [tfidf_results]\n",
    "            if vector_results:\n",
    "                results_list.append(vector_results)\n",
    "                \n",
    "            combined = HybridRetriever.reciprocal_rank_fusion(results_list)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in vector search: {e}\")\n",
    "            # Fallback to TF-IDF results only\n",
    "            combined = tfidf_results\n",
    "        \n",
    "        # Sort by score and limit results\n",
    "        return sorted(combined, key=lambda x: x.get(\"score\", 0), reverse=True)[:top_k]\n",
    "    \n",
    "    def generate_recommendation(self, query: str, top_k: int = 5) -> dict:\n",
    "        \"\"\"Search and generate a response using retrieved results.\"\"\"\n",
    "        logger.info(f\"Generating recommendation for: {query}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Search for relevant chunks\n",
    "        results = self.search(query, top_k=top_k)\n",
    "        \n",
    "        # Generate response using LLM with fallback\n",
    "        response = LLMResponseGenerator.generate_response(query, results)\n",
    "        \n",
    "        # Calculate execution time\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Return comprehensive response object\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"results\": results,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"result_count\": len(results)\n",
    "        }\n",
    "\n",
    "try:\n",
    "    # Path to the chunks and embeddings data - updated to use the smaller files\n",
    "    chunks_path = 'chunks_5000.pkl'\n",
    "    embeddings_path = 'embeddings_5000.pkl'\n",
    "    save_dir = 'saved_objects'\n",
    "    \n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Load chunks\n",
    "    logger.info(f\"Loading chunks from {chunks_path}\")\n",
    "    with open(chunks_path, 'rb') as f:\n",
    "        chunks = pickle.load(f)\n",
    "    \n",
    "    # Load embeddings\n",
    "    logger.info(f\"Loading embeddings from {embeddings_path}\")\n",
    "    with open(embeddings_path, 'rb') as f:\n",
    "        embeddings_data = pickle.load(f)\n",
    "        \n",
    "    embeddings = embeddings_data\n",
    "    \n",
    "    # Initialize recommender\n",
    "    logger.info(\"Creating restaurant recommender\")\n",
    "    recommender = RestaurantRecommender(chunks, embeddings)\n",
    "    \n",
    "    # Save all components\n",
    "    logger.info(\"Saving all components\")\n",
    "    recommender.save_components(save_dir)\n",
    "    \n",
    "    # Test search query\n",
    "    test_query = \"spicy vegetarian food in san francisco\"\n",
    "    logger.info(f\"Testing recommendation for query: {test_query}\")\n",
    "    \n",
    "    recommendation = recommender.generate_recommendation(test_query, top_k=3)\n",
    "    \n",
    "    # Display the recommendation\n",
    "    logger.info(f\"Recommendation for '{test_query}':\")\n",
    "    logger.info(f\"Response: {recommendation['response']}\")\n",
    "    logger.info(f\"Found {recommendation['result_count']} results in {recommendation['execution_time']:.2f} seconds\")\n",
    "    \n",
    "    # Display sample results\n",
    "    for i, res in enumerate(recommendation['results']):\n",
    "        logger.info(f\"Result {i+1}: {res['metadata'].get('restaurant_name', 'Unknown')} - \"\n",
    "                  f\"{res['metadata'].get('menu_item', 'Unknown')} - \"\n",
    "                  f\"Score: {res['score']:.4f}\")\n",
    "    \n",
    "    end = time.time()\n",
    "    logger.info(f\"Total processing time: {(end - start):.2f} seconds\")\n",
    "    \n",
    "    logger.info(\"Process completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in main process: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89350704-b8ab-4f5b-9820-9552cb527097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
